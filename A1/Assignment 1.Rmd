---
title: "Computer Lab 1 block 1"
author: "Kerstin, Tugce and Alan"
date: "2022-11-16"
output: pdf_document
---

## Assignment 1 Handwritten digit recognition with Knearest neighbors.


### 1.1 Data set Partition

The original data set **"optdigits.csv"** has a dimension of **3823 rows and 65
columns.** The last column[65] is the target, and represent which number the each
row is. After dividing the data set with the proportions indicated the number of 
rows for each sub set are: **Training: 1911, Validation: 955 and Test: 957**.
```{r,fig.align='center', echo =FALSE}
## Import the data
df <- read.csv(file = "optdigits.csv", header = FALSE)
n <- dim(df)[1]
n1<- nrow(df) # is the same as above

## Train data 50%
set.seed(12345)
id <- sample(1:n, floor(n*0.5))# floor to take round the numbers
train <- df[id,]
## Validation data 25%
id1<- setdiff(1:n, id) # select data that were not selected on the train data
set.seed(12345)
id2<- sample(id1, floor(n*0.25))
validation <- df[id2,]

## Test data
id3<-setdiff(id1,id2)
test <- df[id3,]

# Plot distribution
nt <- nrow(train)
nv <- nrow(validation)
nte<- nrow(test)

n_data<- c(Training = nt, Validation= nv, Test =nte)
barp<- barplot(n_data, main = "Data density", xlab = "Data set", ylab = "Number of rows",
        col = c("red", "orange", "yellow"))
text(barp, 0, round(n_data, 1),cex=1,pos=3)

```



### 1.2 Training the model (Confusion matrices and Misclasification errors)

The Misclassification rate is calculated as follows:

$\ MR = \frac{FP+FN}{TotalCases}$

- **FP**= False Positives
- **FN** = False Negatives
- **FP + FN**= Incorrect Predictions

The Miscclassification Rate of the Training data set is 4.50 %
The Miscclassification Rate of the Test data set is 5.33 %

From the confusion matrices of Testing and Training data sets, it can be seen that
for both models the numbers with most prediction errors are the number 1,7 and 9.
From the Testing confusion matrix the number 0 has 100% of accuracy, with no
prediction error. In comparison the Training data set has not a 100% of accuracy
in a number, but the "best" predicted number is the number 5, followed by numbers 
6 and 0.

The Misclasification rate indicates that the model with a better quality of the
prediction is the model fitted with the Training data set and test with the same
data set. This result is due to the fact that the model is being tested with data
that it has already "seen", because it was train with it.

```{r, warning=FALSE,echo=FALSE}
library(kknn)
## Fit model with test 
model_1<- kknn(as.factor(V65)~.,train = train, test = test, k =30, kernel = "rectangular")
## Fit model with train
model_2<- kknn(as.factor(V65)~.,train = train, test = train,k=30 ,kernel = "rectangular")
## Predicted values test
pred_test<- model_1$fitted.values
## Predicted values train
pred_train <- model_2$fitted.values
## Confusion matrix 
confusion_matrix_Test <-table(test[,65],pred_test) ## Test
confusion_matrix_Train <-table(train[,65],pred_train) ## Train
confusion_matrix_Test
confusion_matrix_Train
# Calculate the Misclasification rate by a different method Ignore
#m_test <- mean(test[,65] != pred_test)
#m_train <- mean(train[,65] != pred_train)

# cat("Missclasification rate is the sum of False positives + False negatives / Total predictions","\n")
m_test1 <- (sum(test[,65] != pred_test)/length(test[,65]))*100
m_train1 <- (sum(train[,65]!= pred_train)/length(train[,65]))*100
# m_test1
# m_train1
```

### 1.3 Easiest and Hardest cases for digit "8"

The heatmap plot shows that the three cases with lowest probability or the hardest to predict, have shapes which the two circles of the 8 figure is not define in comparison with the easiest to predict that both circles are well define.
Another detail is that the cases with highest probability have darker colors
on the perimeter of the shape, this indicates that there is more pixels in that
position and is easier for the model to identify.

```{r,fig, fig.height = 2, fig.width = 2 ,echo=FALSE}
## Take the rows which target is the number 8
target_train8 <- which(as.factor(train[,65])== 8)
target_test8 <- which(as.factor(test[,65]) == 8)

## Take the probability values of the prediction model for the number 8

prob8_train <- model_2$prob[target_train8,9]#Train
prob8_test<- model_1$prob[target_test8,9] #Test
#We order the indexes we obtain above to take the two lowest and highest
o<-order(prob8_train)
## Train prob
## Hardest
# cat("Hardest","\n")
# prob8_train[50]
# prob8_train[43]
# prob8_train[136]
## Easiest
# cat("Easiest","\n")
# prob8_train[179]
# prob8_train[183]

HL_Prob8 <- matrix(0, nrow = 3, ncol=4)
colnames(HL_Prob8) <- c("Index","Hardest %", "Index", "Easiest %")
HL_Prob8[,1] <- c(50,43,136)
HL_Prob8[,2] <- c(prob8_train[50],prob8_train[43],prob8_train[136])
HL_Prob8[,3] <- c(192,203,NA)
HL_Prob8[,4] <- c(prob8_train[179],prob8_train[183],NA)
print(HL_Prob8)
## Taking the row of the subset of 8's matrix which probability of being an 8 
## is lowest and highest
low1 <- train[target_train8[50],]
low2 <- train[target_train8[43],]
low3 <- train[target_train8[136],]
high1 <- train[target_train8[192],]
high2 <- train[target_train8[203],]

## Reshaping the rows to matrix and taking the last column which is the target or
nlow1 <- matrix(as.numeric(low1[-65]), nrow = 8, ncol = 8, byrow = TRUE)
nlow2 <- matrix(as.numeric(low2[-65]), nrow = 8, ncol = 8, byrow = TRUE)
nlow3 <- matrix(as.numeric(low3[-65]), nrow = 8, ncol = 8, byrow = TRUE)
nhigh1 <- matrix(as.numeric(high1[-65]), nrow = 8, ncol = 8, byrow = TRUE)
nhigh2 <- matrix(as.numeric(high2[-65]), nrow = 8, ncol = 8, byrow = TRUE)

##We could have used matrix and then convert the matrix to the transpose matrix
## Or as we did it, we just set to TRUE the byrow parameter.
## We do this to have the correct orientation on the heatmap(), otherwise we would 
## see an infinite shape instead of the 8's.

## Representation of the numbers 8
cat("Hardest")
heatmap(nlow1, Colv = NA, Rowv = NA)
heatmap(nlow2, Colv = NA, Rowv = NA)
heatmap(nlow3, Colv = NA, Rowv = NA)
cat("Easiest")
heatmap(nhigh1, Colv = NA, Rowv = NA)
heatmap(nhigh2, Colv = NA, Rowv = NA)
```

### 1.4 Model Complexity

After training different models changing the value of k= [1:30], with the training
and validation data. From the plot there is a substantial difference between the
models, the model train and test with the training data show a smaller Misclassification 
error than the Validation model. The complexity of the models increases as the n
umber of K or numbers nearest neighbors decreases.
The optimal K is 4, because it has to be made a trade off between the complexity 
of the model and the Misclassification error. In this case K equal to 4 is the
one with smaller Misclassification rate and without being too small, this means
a model with less complexity. We want to choose a model with lower complexity
to avoid overfitting.

Each model is different and could have different type of data; for example
a model for a health condition is more sensible than one about identifying
types of rocks. With that said, it could be choose a K bigger depending 
of how much impact an small increase on the Misclassification rate creates.
So one could choose a different for the case used in this assignment, an optimal K
equal to 11, because the error difference against K = 4 is around 1 percent.

From the plot it can be seen that K equals to 1 and 2, for the training data set,
resulted in a Misclassification rate equals to 0. In this case with an small
number of K, the model will just identified each observation as only point to consider
while training, so identifies each observation as single case which could lead to over fitting.


```{r, echo=FALSE}
k <- c(1:30)
Knn_models <- matrix(0, nrow = 30, ncol = 4)
colnames(Knn_models) <- c("K","Missclasification rate trainning %",
                          "Missclasification rate validation %",
                          "Missclasification rate test %" )
for (i in k) {
  # Adding values of K to matrix
  Knn_models[i,1] <- i
  # Fit knn models for training and validation data sets with different K <- 1:30
  Knn_train<- kknn(as.factor(V65)~.,train = train, test = train, k =i,
                      kernel = "rectangular")
  knn_validation <- kknn(as.factor(V65)~.,train = train, test = validation,
                         k =i, kernel = "rectangular")
  # Extracting predicted values from the fitted models
  pred_train <- Knn_train$fitted.values
  pred_validation <- knn_validation$fitted.values
  
  # Calculating the Misclacsifications rates
  ms_train <- (mean(train[,65] != pred_train))*100
  ms_validation <- (mean(validation[,65] != pred_validation))*100
 
  # Adding Classifications rates % to the matrix
  Knn_models[i,2] <- ms_train
  Knn_models[i,3] <- ms_validation
  #Knn_models[i,4] <- ms_test
}

plot(k, Knn_models[,2], xlim = rev(range(k)), ylim = c(0,6), col="red", type = "p", 
     main = "Misclassification error vs K", ylab = "Error %", pch=19)
points(k,Knn_models[,3], col="blue", pch=19)
points(30,Knn_models[30,2], col = "red", pch = 19)
text(30,4,label= round(Knn_models[30,2], digits = 3))
points(30,Knn_models[30,3], col= "blue", pch = 19)
text(30,5.7,label= round(Knn_models[30,3], digits = 3))
legend('bottomleft',inset=0.05,c("Train","Validation"),lty=1,col=c("red",
                                    "blue"),title="Data")
```

#### 1.4.1 Misclassification error with Test data set, with optimal K
The error of the model with the Test data set have a similar value to the validation
model, 2.510%(Test) vs 2.513%(Validation). The result is expected because both
data sets have a similar amount of data, 955(validation) vs 957(Test).

```{r, echo=FALSE}
knn_test <- kknn(as.factor(V65)~.,train = train, test = test,
                         k =4, kernel = "rectangular")
pred_test <- knn_test$fitted.values
ms_test <- (mean(test[,65] != pred_test))*100

#pred_test
options(digits=4)
ms_tests <- c(round(Knn_models[4,2], digits=2),Knn_models[4,3], Missclassification_rate_Test= round(ms_test, digits = 2))

ms_tests
```

### 5. Cross entropy for training data

The optimal value of K is 6, because is the one with lowest cross entropy and the 
magnitude of K is not to small to create overfitting.
Cross entropy is a better error performance metric than Misinterpretation rate
for multiclass classification, because cross entropy deals with the total
probability of the targets and gives a better understanding of the whole model.
At then end the aim of using the cross entropy is to minimize the distance
between the two probability distributions.

```{r, echo=FALSE}
## Create a matrix to store the K and Cross entropy values
Knn_models1 <- matrix(0, nrow = 30, ncol = 2)
colnames(Knn_models1) <- c("K","CE")
## Vector to store the probability of the actual target
Individual_prob <- c()
for(i in 1:30){
  ## Store the K value
  Knn_models1[i,1]<-i
  ## Fitting the models
  knn_validation <- kknn(as.factor(V65)~.,train = train, test = validation,
                         k =i, kernel = "rectangular")
  # Probability matrix
  knn_validation$prob
 
   ## Taking probabilities for the value(number) that is supposed to be 
  ## Y[target]-column 65 of dataset partition
  for(j in 1:nrow(knn_validation$prob)){
    ## If the Y[target] is 8, we took the prob of being 8 of the current[i] row
    Individual_prob[j] <- knn_validation$prob[j,validation[j,65]+1]
    }
  ## Cross entropy calculation/ The 
  Knn_models1[i,2] <- -mean(log(Individual_prob + 1e-15))
}
plot(1:30, Knn_models1[,2],xlab = "K", ylab = "Cross entropy", type = "p", pch= 19,
     main = "Dependence of the validation error on the value of k", col= "orange")
points(6,Knn_models1[6,2], col="blue", pch=19)
text(4.7,0.15,label= "K=6")
legend('topright',inset=0.05,c("K"),lty =1,col=c(
                                    "blue"),title="Optimal K")
```

